{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"train/\"\n",
    "testing_path = \"test/\"\n",
    "folders = ['spam','ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildData():\n",
    "    training_dict = {}\n",
    "    testing_dict = {}\n",
    "    for i in range(len(folders)):\n",
    "        training_dict[folders[i]] = os.listdir(training_path + folders[i])\n",
    "        testing_dict[folders[i]] = os.listdir(testing_path + folders[i])\n",
    "    return training_dict,testing_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression(Trn_Val,train_dir,alpha = 0.01):\n",
    "    Word_list = []\n",
    "    Weights = np.zeros((1,161))\n",
    "    b = np.zeros((1,1))\n",
    "    l = 0.1\n",
    "    for class_label, Filelist in list(Trn_Val.items()):\n",
    "        for Filename in Filelist:\n",
    "            with codecs.open(train_dir +'/'+ class_label+ '/'+ Filename, \"r\",encoding='utf-8', errors='ignore') as Filedata:\n",
    "                Word_list += Get_Words(Filedata.read())\n",
    "                x = np.array(Word_list)\n",
    "                y = log_label(class_label)\n",
    "                m = np.shape(x)\n",
    "                Weights = np.arange(1*m[0]).reshape(1, m[0])\n",
    "                b = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                b = np.zeros((1,m[0]))\n",
    "                Weights = Weights.astype(float)\n",
    "                x = x.astype(float)\n",
    "                b = b.astype(float)\n",
    "                for j in range(250):\n",
    "                    z = (Weights*x) + b\n",
    "                    log_exp = 1/(1+np.e**(-z))\n",
    "                    y = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                    error = (-y*np.log(log_exp)+(1-y)*np.log(1-log_exp))\n",
    "                    LS = (error) + (l/2) + (np.square(Weights))\n",
    "                    prod1 = np.array(alpha*LS)\n",
    "                    prod1 = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                    prod1 = prod1.astype(float)\n",
    "                    prod2 = np.array(alpha*l)\n",
    "                    prod2 = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                    prod2 = prod2.astype(float)\n",
    "                    prod3 = prod2 * Weights\n",
    "                    prod3 = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                    prod3 = prod2.astype(float)\n",
    "                    Weights += (prod1) - (prod3)\n",
    "    return Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression_nosw(Trn_Val,train_dir,alpha = 0.01):\n",
    "    W = np.zeros((1,161))\n",
    "    b = np.zeros((1,1))\n",
    "    l = 0.1\n",
    "    Word_list = []\n",
    "    for class_label, Filelist in list(Trn_Val.items()):\n",
    "        for Filename in Filelist:\n",
    "            with codecs.open(train_dir +'/'+ class_label+ '/'+ Filename, \"r\",encoding='utf-8', errors='ignore') as Filedata:\n",
    "                Word_list += Get_Words(Filedata.read())\n",
    "                x = np.array(Word_list)\n",
    "                y = log_label(class_label)\n",
    "                m = np.shape(x)\n",
    "                Weights = np.arange(1*m[0]).reshape(1, m[0])\n",
    "                b = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                b = np.zeros((1,m[0]))\n",
    "                Weights = Weights.astype(float)\n",
    "                x = x.astype(float)\n",
    "                b = b.astype(float)\n",
    "                for j in range(250):\n",
    "                    z = (Weights*x) + b\n",
    "                    log_exp_sw = 1/(1+np.e**(-z))\n",
    "                    y = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                    error = (-y*np.log(log_exp_sw)+(1-y)*np.log(1-log_exp_sw))\n",
    "                    LS = (error) + (l/2) + (np.square(Weights))\n",
    "                    prod1 = np.array(alpha*LS)\n",
    "                    prod1 = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                    prod1 = prod1.astype(float)\n",
    "                    prod2 = np.array(alpha*l)\n",
    "                    prod2 = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                    prod2 = prod2.astype(float)\n",
    "                    prod3 = prod2 * Weights\n",
    "                    prod3 = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                    prod3 = prod2.astype(float)\n",
    "                    Weights += (prod1) - (prod3)\n",
    "    return Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(Tst_Val, test_dir, trained_W,length_test):\n",
    "    b = np.zeros((1,1))\n",
    "    totalCorrectCount = 0\n",
    "    Word_list = []\n",
    "    for act_class, Filelist in list(Tst_Val.items()):\n",
    "        classCorrectCount = 0\n",
    "        for Filename in Filelist:\n",
    "            with codecs.open(test_dir+'/'+ act_class+ '/'+ Filename, \"r\",encoding='utf-8', errors='ignore') as Filedata:\n",
    "                Word_list += Get_Words(Filedata.read())\n",
    "                y = act_class\n",
    "                x = np.array(Word_list)\n",
    "                m = np.shape(x)\n",
    "                trained_W = np.arange(1*m[0]).reshape(1, m[0])\n",
    "                trained_W = trained_W.astype(float)\n",
    "                b = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                b = np.zeros((1,m[0]))\n",
    "                x = x.astype(float)\n",
    "                b = b.astype(float)\n",
    "                prod1 = np.matmul(trained_W,x)\n",
    "                z = (prod1)\n",
    "                prob = np.e**(z)/(1+np.e**(z))\n",
    "                if(prob > 0.5):\n",
    "                    pred_class = 'spam'\n",
    "                else:\n",
    "                    pred_class = 'ham'\n",
    "                if pred_class == act_class:\n",
    "                    classCorrectCount += 1\n",
    "        totalCorrectCount += classCorrectCount\n",
    "        print('The number of ' + act_class + 'predicted correctly', classCorrectCount)\n",
    "    return (float(totalCorrectCount) / length_test) * float(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_nosw(Tst_Val, test_dir, trained_W, length_test):\n",
    "    b = np.zeros((1,1))\n",
    "    totalCorrectCount = 0\n",
    "    Word_list = []\n",
    "    for act_class, Filelist in list(Tst_Val.items()):\n",
    "        classCorrectCount = 0\n",
    "        for Filename in Filelist:\n",
    "            with codecs.open(test_dir+'/'+ act_class+ '/'+ Filename, \"r\",encoding='utf-8', errors='ignore') as Filedata:\n",
    "                Word_list += delete_sw(Filedata.read())\n",
    "                x = np.array(Word_list)\n",
    "                m = np.shape(x)\n",
    "                trained_W = np.arange(1*m[0]).reshape(1, m[0])\n",
    "                trained_W = trained_W.astype(float)\n",
    "                b = np.arange(1*m[0]).reshape(1,m[0])\n",
    "                b = np.zeros((1,m[0]))\n",
    "                x = x.astype(float)\n",
    "                b = b.astype(float)\n",
    "                prod1 = np.matmul(trained_W,x)\n",
    "                z = (prod1)\n",
    "                prob = np.e**(z)/(1+np.e**(z))\n",
    "                if(prob > 0.5):\n",
    "                    pred_class = 'spam'\n",
    "                else:\n",
    "                    pred_class = 'ham'\n",
    "                if pred_class == act_class:\n",
    "                    classCorrectCount += 1\n",
    "        totalCorrectCount += classCorrectCount\n",
    "        print('The number of ' + act_class + ' predicted correctly', classCorrectCount)\n",
    "    return (float(totalCorrectCount) / length_test) * float(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Words(text):\n",
    "    words = text.strip().split()\n",
    "    for i in range(len(words)-1):\n",
    "        words += words[i] + ' ' + words[i + 1]\n",
    "    words = set(words)\n",
    "    words = list(words)\n",
    "    df_num = pd.DataFrame({'col':words})\n",
    "    df_num.col = pd.Categorical(df_num.col)\n",
    "    df_num['cat_code'] = df_num.col.cat.codes\n",
    "    a = df_num.cat_code\n",
    "    s = list(a)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_sw(text):\n",
    "    words = text.strip().split()\n",
    "    for i in range(len(words)-1):\n",
    "        words += words[i] + ' ' + words[i + 1]\n",
    "    words = set(words)\n",
    "    word_list = []\n",
    "    for word in words:\n",
    "        if word not in stop_wd:\n",
    "            word_list.append(word) \n",
    "    word_list = list(word_list)\n",
    "    df_num = pd.DataFrame({'col':word_list})\n",
    "    df_num.col = pd.Categorical(df_num.col)\n",
    "    df_num['cat_code'] = df_num.col.cat.codes\n",
    "    a = df_num.cat_code\n",
    "    s = list(a)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_label(label):\n",
    "    if label == 'spam':\n",
    "        y = 1\n",
    "    else:\n",
    "        y = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Total_docs(trainData):\n",
    "    Sum = 0 \n",
    "    for i in trainData.keys():\n",
    "        Sum = Sum + len(trainData[i])\n",
    "    return Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trn_Val, Tst_Val = Split(Trn_dir,Tst_dir)\n",
    "length_test = Total_docs(Tst_Val)\n",
    "print(\"With stop words\")\n",
    "print(\"-----------------\\n\\n\")\n",
    "trained_Weights = Logistic_Regression(Trn_Val,Trn_dir,alpha=0.01)\n",
    "accuracy = result(Tst_Val,Tst_dir,trained_Weights,length_test)\n",
    "print(\"Accuracy of Logistic Regression with Stop Words:\",accuracy)\n",
    "print(\"\\nWithout stop words\")\n",
    "print(\"-----------------\\n\\n\")\n",
    "trained_Weights_nosw = Logistic_Regression_nosw(Trn_Val,Trn_dir,0.01)\n",
    "accuracy_sw = result_nosw(Tst_Val,Tst_dir,trained_Weights_nosw,length_test)\n",
    "print(\"Accuracy with Logistic Regression without Stop Words:\",accuracy_sw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
